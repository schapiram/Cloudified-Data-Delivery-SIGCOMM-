\subsection{KTCP Design} \label{sec:design}
\subsubsection{Design Goals}
The aim of KTCP split is to provide uncompromising TCP optimization, while, utilizing commodity VMs. Easily deploy-able commodity VMs in a public cloud environment, provide a powerful and flexible platform for our goals. We implement KTCP as a kernel module for Ubuntu 16.04. We use the ubiquitous POSIX socket API as the basis for our TCP proxy.  
Implementing KTCP in the kernel allows us; to take advantage of resources only available in the kernel and avoid the penalties that stem from numerous system calls ~\cite{Copy, FlexSC}. We add several proc ~\cite{proc} files to control the behaviour of KTCP. 

\T{Basic implementation} The basic design of our TCP proxy, shares a common form with other open source implementations ~\cite{SOCKS, WhatEverAranIsUsing}. The design has three basic steps. We create a socket that listens for incoming connections. Iptable ~\cite{iptables} rules redirect TCP packets to our proxy socket. A second TCP socket is used to connect to the destination. All bytes of the stream are read from one socket and then forwarded to its peer.

\T{Early Syn} In early syn ~\cite{Ladiwala}, a syn packet is sent to the next-hop server as soon as the syn packet arrives. This without waiting for the three way handshake to finish. Standard POSIX socket API doesn't facilitate the capture of the first TCP SYN packet. We utilize Linux netfilter~\cite{netfilter} hooks to captures this first SYN packet and trigger the start of a new connection. This allows the proxy to establish the two connection in parallel. 

\T{Thread Pool} Each new proxy connection is handled within two dedicated kernel\_threads. Each thread receives from one socket and writes to its peer. Unfortunately, the creation of a new kernel\_thread is costly on our setup it takes XXX milliseconds\MA{I plan to run a small experiment to measure the time it takes to create a kernel\_thread, a fork and a POSIX pthread}. This delay is added to the time to first to byte. To mitigate this problem, we create a pool of reusable kernel threads. We create a list of pre-allocated kernel\_threads. Each kernel\_thread is initially in TASK\_INTERRUPTIBLE state; when the thread is allocated the task is scheduled to run and a function to run is set , when the function is completed the thread return to state TASK\_INTERRUPTIBLE; awaiting for a new function to run \MA{awkward needs a rewrite}.A pool of pre-allocated kernel threads thus removes the overhead of new kernel\_thread creation, as a new connection initialization can start immediately.

\T{Reusable connections} Another optimization we implement, aims to eliminate the delay of the three way handshake between two proxies. We create a pool of connections between two distant proxies. The sockets are configured with KEEP\_ALIVE. On early syn we allocate a connected socket from our pool and use it for the connection; thus eliminating the three way handshake time. Before we start forwarding the data we send the destination address on the connection it self, all following bytes belong to the original stream. 
This element has a server and a client part. The server has a dedicated socket that accepts connections from peer-proxies. These connections wait for for the destination address, when the destination address is received a connection is established and the bytes are forwarded between the sockets just like in the basic design. Nagel's Algorithm ~\cite{nagel} should be disabled on these sockets; sockets configured TCP\_NODELAY. In our experiments we have seen that; without this configuration, time to first is increased by 200 milliseconds.  

\T{Design Considerations} While most optimizations we propose in KTCP, can be implemented in user space, and thus provide an easier development environment. Netfilter ~\cite{netfilter} is the natural option for capturing the syn packet and this option is only available via a kernel module. Additionally, numerous system calls can hinder performance considerably ~\cite{Copy, FlexSC}\MA{An experiment that shows this could be beneficial}. By working in the kernel, we eliminate the redundant transitions to and from user space. The decision to implement KTCP in the kernel is farther made easy, by the fact that, all socket API have a kernel counterpart. One problem with an in kernel implementation is that epoll ~\cite{epoll} has no kernel API. We believe that context switching between kernel\_threads may become an issue with scale. We evaluate the context switch time and find that it takes YYY Yseconds \MA{Need to run this test}.

\T{Proc} The size of the thread-pool, the destination of a pre-connection and the number of pre-connections are controlled via the proc fs~\cite{proc} interface.

\T{Extreme performance} \MA{This paragraph is just my ramblings on performance, this whole discussion is gratuitous}For a very large number of connections it may become necessary to expand the epoll API to the kernel. Socket API in the kernel is not zero copy, expanding the socket API is possible but not trivial, copying data can become an issue with scale ~\cite{Copy}. Network I/O is serviced with interrupts, on virtual machines this usually means costly VM exits ~\cite{Eli, Elvis}. Additionally it is well documented that para-virtual devices like ~\cite{virtio,vmxnet3} have a penalty to performance ~\cite{Eli, Elvis}. An SRIOV device and a machine with a CPU that supports Intels vt-d posted interrupts may be needed to achieve near bear metal performance.
