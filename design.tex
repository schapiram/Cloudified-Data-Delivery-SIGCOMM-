\subsection{Rate-Control Interface: KTCP Design} \label{sec:design}

\IK{Alex:Sorry for the many comments, feel free to disregard if irrelevant.}

\IK{How about we start by listing the main design goals and the resulting choices that are the most significant and differ from the literature? For instance: \\
1. Use commodity VMs, unlike [refs]\\
2. use most efficient implementation and therefore kernel mode because...\\
3. Use most standard implementation and therefore POSIX because...\\
etc.} \IK{Also: make sure you mention the things you explored and didn't work if there is something interesting to learn...}

\T{Design choices/goals.}

The aim of KTCP split is to provide uncompromising TCP optimization, \IK{maybe replace ``uncompromising TCP optimization"  by something like ``provide a delay- and throughput-efficient TCP proxy interface between the non-cloud and cloud parts of the data transfer,"} while utilizing commodity VMs. Easily-deployable commodity VMs in a public cloud environment provide a powerful and flexible platform for our goals. 

\T{Implementation environment.} We implement KTCP as a kernel module for Ubuntu 16.04. We use the ubiquitous POSIX socket API \IK{aka BSD socket API?} as the basis for our TCP proxy.  

\T{Kernel mode.} \IK{Merge the next two paragraphs?} Implementing KTCP in the kernel allows us; to take advantage of resources only available in the kernel and avoid the penalties that stem from numerous system calls ~\cite{Copy, FlexSC}. \IK{What do we gain? For instance latency because of reduced interrupts, or throughput, etc.? Also what do we lose? Are these potential security breaches? Bear in mind that the typical reader knows little about computer architecture, although probably at least one reviewer will be super knowledgeable.} We add several proc ~\cite{proc} files to control the behaviour of KTCP. \IK{please expand; e.g., ``the proc file system contains a hierarchy of special files that represent the current state of the kernel. We add the following files:..."; I later saw you have a line at the bottom...}

%\T{Design Considerations} 
While most optimizations we propose in KTCP can be implemented in user space, and thus provide an easier development environment. Netfilter ~\cite{netfilter} is the natural option for capturing the syn packet and this option is only available via a kernel module. Additionally, numerous system calls can hinder performance considerably ~\cite{Copy, FlexSC}\MA{An experiment that shows this could be beneficial}. By working in the kernel, we eliminate the redundant transitions to and from user space. The decision to implement KTCP in the kernel is farther made easy, by the fact that, all socket API have a kernel counterpart. One problem with an in kernel implementation is that epoll ~\cite{epoll} has no kernel API. We believe that context switching between kernel\_threads may become an issue with scale. We evaluate the context switch time and find that it takes YYY Yseconds \MA{Need to run this test}.

\T{Basic implementation.} The basic design of our TCP proxy shares a common form with other open-source implementations ~\cite{SOCKS, WhatEverAranIsUsing}. The design has three basic steps. We create a socket that listens for incoming connections. Iptable ~\cite{iptables} rules redirect TCP packets to our proxy socket. A second TCP socket is used to connect to the destination. All bytes of the stream are read from one socket and then forwarded to its peer.

\T{Improvement 1: early SYN} In early SYN ~\cite{Ladiwala}, a syn packet is sent to the next-hop server as soon as the syn packet arrives. This is done without waiting for the three-way handshake to finish. The standard POSIX socket API doesn't facilitate the capture of the first TCP SYN packet. \IK{Why? What's the challenge?\\ Also do we know of any open-source implementation like ~\cite{SOCKS, WhatEverAranIsUsing} with early SYN, or can we tell that we don't know of any? For instance, any idea what the unikernel guys used if not POSIX?} We utilize Linux netfilter~\cite{netfilter} hooks \IK{details?} to capture this first SYN packet and trigger the start of a new connection. This allows the proxy to establish the two connections in parallel. \IK{BTW, for each of the improvements, is it possible to give a rough estimate of the number of lines of code involved? If not, that's fine too.}

\T{Improvement 2: thread pool} Each new proxy connection is handled within two dedicated kernel\_threads.  Each thread receives from one socket and writes to its peer. Unfortunately, the creation of a new kernel\_thread is costly on our setup it takes XXX milliseconds\MA{I plan to run a small experiment to measure the time it takes to create a kernel\_thread, a fork and a POSIX pthread}.\IK{Sounds great -- getting back to the comment above: reviewers will just say that we could have done it in user mode, and just created an artificial problem; we need to convince why user mode is not good.} \IK{BTW I called this time the fork time, let me know if you prefer a different name.} This delay is added to the time to first byte (TTFB) \IK{well, also to the download time, etc.; maybe just delete this sentence?}. 

To mitigate this problem, we create a pool of reusable kernel threads \IK{``, which are essentially empty threads that are waiting to be assigned to a specific connection"?}. We create a list of pre-allocated kernel\_threads. Each kernel\_thread is initially in TASK\_INTERRUPTIBLE state. When the thread is allocated, the task is scheduled to run and a function to run is set. When the function is completed, the thread return to state TASK\_INTERRUPTIBLE, awaiting for a new function to run \MA{awkward needs a rewrite}. A pool of pre-allocated kernel threads thus removes the overhead of new kernel\_thread creation, as a new connection initialization can start immediately. \IK{Mention the garbage collection or there isn't?}

\T{Improvement 3: reusable connections} \IK{Need to rephrase the  sentence about the problem, and add a sentence about the fact that we use the fact that there is a small number $n$  of routers, so only need for instance $5 n(n-1)$ empty connections to get an empty connection in each connection, and also the fact that these empty connections are not expensive (because keep alive only works every x milliseconds)}
Another optimization we implement aims to eliminate the delay of the three way handshake between two proxies. We create a pool of connections between two distant proxies. The sockets are configured with KEEP\_ALIVE. On early SYN, we allocate a connected socket from our pool and use it for the connection; thus eliminating the three-way handshake time. 

\IK{Again mention the challenge you face before you give the solution}
Before we start forwarding the data we send the destination address on the connection itself, all following bytes belong to the original stream. 
This element has a server and a client part. The server has a dedicated socket that accepts connections from peer-proxies. These connections wait for for the destination address, when the destination address is received a connection is established and the bytes are forwarded between the sockets just like in the basic design. 
Nagel's Algorithm ~\cite{nagel} should be disabled on these sockets \IK{why? remind the bad thing it does...}; sockets configured TCP\_NODELAY. In our experiments we have seen that; without this configuration, time to first byte is increased by 200 milliseconds.  


\T{Proc} The size of the thread-pool, the destination of a pre-connection and the number of pre-connections are controlled via the proc fs~\cite{proc} interface.

\IK{Below: Can you give an order of magnitude of the scae at which we would need a different type of implementation?\\ Also would this fit into a discussion section, or should it be here because it helps to understand implementation tradeoffs?}

\T{Extreme performance} \MA{This paragraph is just my ramblings on performance, this whole discussion is gratuitous}For a very large number of connections it may become necessary to expand the epoll API to the kernel. Socket API in the kernel is not zero copy, expanding the socket API is possible but not trivial, copying data can become an issue with scale ~\cite{Copy}. Network I/O is serviced with interrupts, on virtual machines this usually means costly VM exits ~\cite{Eli, Elvis}. Additionally it is well documented that para-virtual devices like ~\cite{virtio,vmxnet3} have a penalty to performance ~\cite{Eli, Elvis}. An SRIOV device and a machine with a CPU that supports Intel's vt-d posted interrupts may be needed to achieve near bare-metal performance.
